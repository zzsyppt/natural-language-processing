{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c537b7d6",
   "metadata": {},
   "source": [
    "### 文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb2c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    " \n",
    "Path(\"cache\").mkdir(exist_ok=True)\n",
    "\n",
    " \n",
    "ARTICLE_DIR = \"article\"\n",
    "TOKENIZED_PATH = \"cache/tokenized_docs.json\"\n",
    "WORD_LIST_PATH = \"cache/word_list.json\"\n",
    "FAISS_INDEX_PATH = \"cache/faiss.index\"\n",
    "SQLITE_VECTOR_DB = \"cache/word_vectors.db\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935ad46",
   "metadata": {},
   "source": [
    "### 文章加载+分词（多线程处理+缓存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef286e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到缓存文件，正在加载分词结果...\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, jieba \n",
    "from tqdm import tqdm   \n",
    "import time\n",
    "\n",
    "# 加载所有文档，文件名为ID\n",
    "def load_documents(folder_path):\n",
    "    docs = {}\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            doc_id = int(filename.replace(\".txt\", \"\"))\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                docs[doc_id] = f.read()\n",
    "    return docs\n",
    "\n",
    "# 分词 + 清洗\n",
    "def jieba_cut_serial(text):\n",
    "    words = list(jieba.cut(text))\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if len(word) <= 1:\n",
    "            continue\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        if re.match(r\"^[^\\w\\u4e00-\\u9fa5]+$\", word):\n",
    "            continue\n",
    "        clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "# 主逻辑\n",
    "def tokenize_serial(documents, cache_path=TOKENIZED_PATH):\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"r\", encoding='utf-8') as f:\n",
    "            print(\"检测到缓存文件，正在加载分词结果...\")\n",
    "            return json.load(f)\n",
    "\n",
    "    print(\"正在进行分词...\")\n",
    "    ids, texts = zip(*documents.items())\n",
    "    tokenized = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for doc_id, text in tqdm(zip(ids, texts), total=len(ids), desc=\"分词进度\"):\n",
    "        tokenized[doc_id] = jieba_cut_serial(text)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    with open(cache_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(tokenized, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"分词完成，共{len(tokenized)}篇文档，耗时{elapsed:.2f}秒\")\n",
    "    return tokenized\n",
    "\n",
    "documents = load_documents(ARTICLE_DIR) \n",
    "tokenized_docs = tokenize_serial(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8657d36",
   "metadata": {},
   "source": [
    "### BERT模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1a3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zzsyp\\anaconda3\\envs\\lanxin\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\zzsyp\\anaconda3\\envs\\lanxin\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT模型已加载到GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch \n",
    "\n",
    "class ChineseWordEncoder:\n",
    "    def __init__(self, model_name=\"hfl/chinese-roberta-wwm-ext\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        # 加载预训练模型并将其放到GPU上\n",
    "        self.model = BertModel.from_pretrained(model_name).to(\"cuda\")\n",
    "        # 设置为评估模式\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        # 使用 tokenizer 对词语进行编码，生成 token id 形式的输入\n",
    "        # truncation=True 表示对过长输入自动截断；max_length=10 是为了安全限制长度\n",
    "        # return_tensors=\"pt\" 会返回 PyTorch 张量形式的输入 \n",
    "        inputs = self.tokenizer(word, return_tensors=\"pt\", truncation=True, max_length=10).to(\"cuda\")\n",
    "        # 关闭梯度计算（加速 + 减少显存占用），执行前向传播，获取输出结果\n",
    "        with torch.no_grad():\n",
    "            # outputs 是一个包含 last_hidden_state 等多个输出的对象\n",
    "            outputs = self.model(**inputs)\n",
    "        # 从输出中提取 `last_hidden_state`，形状为 (batch_size=1, sequence_len, hidden_dim)\n",
    "        # 我们选取第一个 token（CLS 标记）对应的向量表示，作为整体词语的语义表达\n",
    "        cls_vector = outputs.last_hidden_state[:, 0, :]  # 取 batch 的第一个 token 的全部隐藏层向量\n",
    "\n",
    "        # squeeze(0)：将 (1, 768) => (768,)\n",
    "        # cpu().numpy()：将张量从 GPU 移至 CPU 并转为 NumPy 数组\n",
    "        return cls_vector.squeeze(0).cpu().numpy()\n",
    "\n",
    "encoder = ChineseWordEncoder()\n",
    "print(\"BERT模型已加载到GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef71a42",
   "metadata": {},
   "source": [
    "### SQLite数据库有关方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6253763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# 初始化 SQLite 数据库\n",
    "def init_sqlite_vector_db(db_path=SQLITE_VECTOR_DB):\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_vectors (\n",
    "            word TEXT PRIMARY KEY,\n",
    "            vector BLOB\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# 写入词向量\n",
    "def save_vector_to_sqlite(word, vector, db_path=SQLITE_VECTOR_DB):\n",
    "    vec_bytes = vector.astype(\"float32\").tobytes()\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"INSERT OR REPLACE INTO word_vectors (word, vector) VALUES (?, ?)\", (word, vec_bytes))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# 读取词向量\n",
    "def load_vector_from_sqlite(word, db_path=SQLITE_VECTOR_DB):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT vector FROM word_vectors WHERE word = ?\", (word,))\n",
    "    row = c.fetchone()\n",
    "    conn.close()\n",
    "    if row is None:\n",
    "        return None\n",
    "    return np.frombuffer(row[0], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8225f",
   "metadata": {},
   "source": [
    "### 倒排索引 + 跳表交集 + SQLite + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee591aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "from collections import defaultdict\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(list) \n",
    "        self.word_list = [] \n",
    "        self.faiss_index = None\n",
    "\n",
    "    def build(self, tokenized_docs, encoder, db_path=SQLITE_VECTOR_DB):\n",
    "        # 如果缓存文件都存在，直接加载\n",
    "        if os.path.exists(WORD_LIST_PATH) and os.path.exists(FAISS_INDEX_PATH):\n",
    "            with open(WORD_LIST_PATH, \"r\", encoding='utf-8') as f:\n",
    "                self.word_list = json.load(f)\n",
    "            self.faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "            print(f\"已从缓存加载 word_list 和 faiss.index，词数：{len(self.word_list)}\")\n",
    "            \n",
    "            # 同时仍需重建倒排索引（来自 tokenized_docs）\n",
    "            for doc_id, tokens in tokenized_docs.items():\n",
    "                for token in set(tokens):\n",
    "                    self.index[token].append(int(doc_id))\n",
    "            for token in self.index:\n",
    "                self.index[token].sort()\n",
    "            return  # 直接结束，避免后续冗余步骤\n",
    "        \n",
    "        # 初始化 SQLite 数据库\n",
    "        init_sqlite_vector_db(db_path)\n",
    "\n",
    "        # 1. 构建倒排索引 + 词表\n",
    "        vocab = set()\n",
    "        for doc_id, tokens in tokenized_docs.items():\n",
    "            # set() 去重\n",
    "            for token in set(tokens):\n",
    "                self.index[token].append(int(doc_id))\n",
    "                vocab.add(token)\n",
    "        self.word_list = list(self.vocab)\n",
    "\n",
    "        # 2. 向量构建并写入 SQLite，同时构建向量集合用于 FAISS\n",
    "        vecs = []\n",
    "        word_vectors = {}\n",
    "        for word in self.word_list:\n",
    "            vec = load_vector_from_sqlite(word, db_path)\n",
    "            if vec is None:\n",
    "                try:\n",
    "                    vec = encoder.get_vector(word)\n",
    "                    save_vector_to_sqlite(word, vec, db_path)\n",
    "                except:\n",
    "                    vec = np.zeros(768)\n",
    "                    save_vector_to_sqlite(word, vec, db_path)\n",
    "            word_vectors[word] = vec.tolist()\n",
    "            vecs.append(vec)\n",
    "\n",
    "        # 3. 构建 FAISS 索引\n",
    "        vecs = np.array(vecs).astype(\"float32\")\n",
    "        self.faiss_index = faiss.IndexFlatL2(vecs.shape[1])\n",
    "        self.faiss_index.add(vecs)\n",
    "\n",
    "        # 4. 保存 word_list 和 FAISS 索引\n",
    "        with open(WORD_LIST_PATH, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(self.word_list, f)\n",
    "        faiss.write_index(self.faiss_index, FAISS_INDEX_PATH)\n",
    "\n",
    "        # 5. 对倒排索引按文档排序\n",
    "        for token in self.index:\n",
    "            self.index[token].sort()\n",
    "\n",
    "        print(f\"倒排索引构建完成，共索引词数：{len(self.word_list)}，词向量缓存于SQLite。\")\n",
    "    \n",
    "    \n",
    "    def intersect_with_skip(self, list1, list2):\n",
    "        if not list1 or not list2:\n",
    "            return []\n",
    "\n",
    "        result = []\n",
    "        i, j = 0, 0\n",
    "        len1, len2 = len(list1), len(list2)\n",
    "        skip1 = int(len1 ** 0.5) or 1\n",
    "        skip2 = int(len2 ** 0.5) or 1\n",
    "\n",
    "        while i < len1 and j < len2:\n",
    "            if list1[i] == list2[j]:\n",
    "                result.append(list1[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif list1[i] < list2[j]:\n",
    "                next_i = i + skip1\n",
    "                if next_i < len1 and list1[next_i] <= list2[j]:\n",
    "                    i = next_i\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                next_j = j + skip2\n",
    "                if next_j < len2 and list2[next_j] <= list1[i]:\n",
    "                    j = next_j\n",
    "                else:\n",
    "                    j += 1\n",
    "\n",
    "        return result\n",
    "    \n",
    "    # Top-K相似词查询函数\n",
    "    def top_k_similar(self, query_vec, query_word=None, k=2):\n",
    "        query_vec = np.array([query_vec]).astype(\"float32\")\n",
    "        distances, indices = self.faiss_index.search(query_vec, k + 1)\n",
    "        candidates = []\n",
    "        for i, d in zip(indices[0], distances[0]):\n",
    "            word = self.word_list[i]\n",
    "            if word == query_word:\n",
    "                continue\n",
    "            candidates.append((word, d))\n",
    "            if len(candidates) == k:\n",
    "                break\n",
    "        return candidates\n",
    "\n",
    "    # 搜索+TOP K查询函数\n",
    "    def search(self, word, encoder, topk=2):\n",
    "        start_time = time.time()\n",
    "        docs_word = self.index.get(word, [])\n",
    "        try:\n",
    "            word_vec = encoder.get_vector(word)\n",
    "        except:\n",
    "            return [], [], [], 0.0\n",
    "\n",
    "        top_similar = self.top_k_similar(word_vec, word, k=topk)\n",
    "        docs_similars = [self.index.get(sim_word, []) for sim_word, _ in top_similar]\n",
    "        if len(docs_similars) < 2:\n",
    "            return [], docs_word, top_similar, time.time() - start_time\n",
    "\n",
    "        all_lists = [docs_word] + docs_similars\n",
    "        all_lists = sorted(all_lists, key=len)\n",
    "        temp = self.intersect_with_skip(all_lists[0], all_lists[1])\n",
    "        final_result = self.intersect_with_skip(temp, all_lists[2])\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"\\n=== 🔎查询关键词：{word} ===\")\n",
    "        print(f\"查询词包含文档数: {len(docs_word)}，示例:{docs_word[:10]}\")\n",
    "        print(f\"Top{topk}相似词:\")\n",
    "        for i, (sim_word, dist) in enumerate(top_similar, 1):\n",
    "            sim_docs = self.index.get(sim_word, [])\n",
    "            print(f\" {i}. {sim_word} (L2距离{dist:.2f})，出现于{len(sim_docs)}篇文档，示例:{sim_docs[:10]}\")\n",
    "        print(f\"交集文档数: {len(final_result)}，示例:{final_result[:10]}\")\n",
    "        print(f\"查询耗时: {elapsed:.3f}秒\")\n",
    "\n",
    "        return final_result, docs_word, top_similar, elapsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65404d45",
   "metadata": {},
   "source": [
    "### 测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670c73f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已从缓存加载 word_list 和 faiss.index，词数：88370\n",
      "\n",
      "=== 🔎查询关键词：互联网 ===\n",
      "查询词包含文档数: 439，示例:[8, 93, 106, 122, 126, 163, 180, 221, 235, 251]\n",
      "Top2相似词:\n",
      " 1. 互联网络 (L2距离28.31)，出现于2篇文档，示例:[15433, 19630]\n",
      " 2. 互联网站 (L2距离28.79)，出现于1篇文档，示例:[9514]\n",
      "交集文档数: 0，示例:[]\n",
      "查询耗时: 0.573秒\n",
      "\n",
      "=== 🔎查询关键词：经济 ===\n",
      "查询词包含文档数: 3040，示例:[2, 5, 13, 28, 29, 30, 32, 44, 47, 65]\n",
      "Top2相似词:\n",
      " 1. 非经济 (L2距离40.02)，出现于1篇文档，示例:[17892]\n",
      " 2. 经济账 (L2距离40.52)，出现于2篇文档，示例:[10483, 18906]\n",
      "交集文档数: 0，示例:[]\n",
      "查询耗时: 0.033秒\n",
      "\n",
      "=== 🔎查询关键词：美国 ===\n",
      "查询词包含文档数: 1752，示例:[28, 39, 62, 67, 77, 110, 126, 136, 140, 152]\n",
      "Top2相似词:\n",
      " 1. 美等国 (L2距离36.43)，出现于1篇文档，示例:[29]\n",
      " 2. 美两国 (L2距离39.21)，出现于4篇文档，示例:[4373, 16508, 18173, 18192]\n",
      "交集文档数: 0，示例:[]\n",
      "查询耗时: 0.035秒\n",
      "\n",
      "=== 🔎查询关键词：消费 ===\n",
      "查询词包含文档数: 265，示例:[90, 140, 250, 311, 381, 395, 415, 513, 538, 547]\n",
      "Top2相似词:\n",
      " 1. 消费观 (L2距离43.97)，出现于1篇文档，示例:[10196]\n",
      " 2. 消费量 (L2距离44.27)，出现于8篇文档，示例:[3433, 4317, 10131, 10196, 11781, 11782, 17302, 18685]\n",
      "交集文档数: 0，示例:[]\n",
      "查询耗时: 0.035秒\n",
      "\n",
      "=== 🔎查询关键词：军队 ===\n",
      "查询词包含文档数: 396，示例:[2, 5, 91, 106, 144, 156, 178, 308, 453, 528]\n",
      "Top2相似词:\n",
      " 1. 军人 (L2距离31.02)，出现于124篇文档，示例:[453, 549, 605, 643, 655, 673, 762, 800, 1627, 1778]\n",
      " 2. 部队 (L2距离31.51)，出现于477篇文档，示例:[1, 2, 15, 91, 144, 178, 183, 316, 371, 404]\n",
      "交集文档数: 10，示例:[453, 3818, 3887, 3919, 3960, 7723, 11399, 13479, 16498, 20654]\n",
      "查询耗时: 0.033秒\n"
     ]
    }
   ],
   "source": [
    "index = InvertedIndex()\n",
    "index.build(tokenized_docs, encoder)\n",
    "\n",
    "query_words = [\"互联网\", \"经济\", \"美国\", \"消费\", \"军队\"]\n",
    "results = {}\n",
    "\n",
    "for query in query_words:\n",
    "    result_docs, base_docs, top2, used_time = index.search(query, encoder)\n",
    "    results[query] = result_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3cbeee",
   "metadata": {},
   "source": [
    "### 功能扩展 复杂布尔表达式查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604de47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优先级设定\n",
    "PRECEDENCE = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "\n",
    "def tokenize(expr):\n",
    "    # 括号前后加空格，方便分隔\n",
    "    expr = expr.replace('(', ' ( ').replace(')', ' ) ')\n",
    "    return expr.strip().split()\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    output = []\n",
    "    stack = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()  # 弹出 (\n",
    "        elif token.upper() in PRECEDENCE:\n",
    "            while (stack and stack[-1] != '(' and\n",
    "                   PRECEDENCE.get(stack[-1].upper(), 0) >= PRECEDENCE[token.upper()]):\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token.upper())\n",
    "        else:\n",
    "            output.append(token)\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "def eval_postfix(postfix_tokens, index, all_ids):\n",
    "    stack = []\n",
    "    for token in postfix_tokens:\n",
    "        token = token.upper()\n",
    "        if token == 'NOT':\n",
    "            if not stack:\n",
    "                raise ValueError(\"栈为空，NOT 缺少操作数\")\n",
    "            operand = stack.pop()\n",
    "            stack.append(all_ids - operand)\n",
    "        elif token in ('AND', 'OR'):\n",
    "            b = stack.pop()\n",
    "            a = stack.pop()\n",
    "            result = a & b if token == 'AND' else a | b\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            stack.append(set(index.index.get(token, [])))\n",
    "    if len(stack) != 1:\n",
    "        raise ValueError(\"后缀表达式求值错误！检查表达式\")\n",
    "    return stack[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053e0662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入表达式: NOT 经济 AND (互联网 AND 消费) \n",
      "后缀表达式: ['经济', 'NOT', '互联网', '消费', 'AND', 'AND']\n",
      "匹配文档数: 4\n",
      "文档ID样例: [19658, 18827, 15702, 18847]\n",
      "输入表达式: 经济 AND 互联网 AND 消费\n",
      "后缀表达式: ['经济', '互联网', 'AND', '消费', 'AND']\n",
      "匹配文档数: 21\n",
      "文档ID样例: [18179, 1672, 18700, 6960, 14388, 5430, 15418, 16700, 3260, 1472]\n",
      "输入表达式: 互联网 AND 消费\n",
      "后缀表达式: ['互联网', '消费', 'AND']\n",
      "匹配文档数: 25\n",
      "文档ID样例: [18179, 1672, 18827, 18700, 18847, 6960, 14388, 5430, 15418, 16700]\n"
     ]
    }
   ],
   "source": [
    "exprs = [\"NOT 经济 AND (互联网 AND 消费) \" ,\n",
    "         \"经济 AND 互联网 AND 消费\",\n",
    "         \"互联网 AND 消费\"\n",
    "]\n",
    "# 构造全集\n",
    "all_ids = set()\n",
    "for docs in index.index.values():\n",
    "    all_ids.update(docs)\n",
    "\n",
    "for expr in exprs:\n",
    "    print(\"输入表达式:\", expr)\n",
    "    tokens = tokenize(expr)\n",
    "    postfix = infix_to_postfix(tokens)\n",
    "    print(\"后缀表达式:\", postfix) \n",
    "    result = eval_postfix(postfix, index, all_ids)\n",
    "    print(\"匹配文档数:\", len(result))\n",
    "    print(\"文档ID样例:\", list(result)[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lanxin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
