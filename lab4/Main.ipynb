{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c537b7d6",
   "metadata": {},
   "source": [
    "### æ–‡ä»¶è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb2c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    " \n",
    "Path(\"cache\").mkdir(exist_ok=True)\n",
    "\n",
    " \n",
    "ARTICLE_DIR = \"article\"\n",
    "TOKENIZED_PATH = \"cache/tokenized_docs.json\"\n",
    "WORD_LIST_PATH = \"cache/word_list.json\"\n",
    "FAISS_INDEX_PATH = \"cache/faiss.index\"\n",
    "SQLITE_VECTOR_DB = \"cache/word_vectors.db\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935ad46",
   "metadata": {},
   "source": [
    "### æ–‡ç« åŠ è½½+åˆ†è¯ï¼ˆå¤šçº¿ç¨‹å¤„ç†+ç¼“å­˜ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef286e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½åˆ†è¯ç»“æœ...\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, jieba \n",
    "from tqdm import tqdm   \n",
    "import time\n",
    "\n",
    "# åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼Œæ–‡ä»¶åä¸ºID\n",
    "def load_documents(folder_path):\n",
    "    docs = {}\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            doc_id = int(filename.replace(\".txt\", \"\"))\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                docs[doc_id] = f.read()\n",
    "    return docs\n",
    "\n",
    "# åˆ†è¯ + æ¸…æ´—\n",
    "def jieba_cut_serial(text):\n",
    "    words = list(jieba.cut(text))\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if len(word) <= 1:\n",
    "            continue\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        if re.match(r\"^[^\\w\\u4e00-\\u9fa5]+$\", word):\n",
    "            continue\n",
    "        clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "# ä¸»é€»è¾‘\n",
    "def tokenize_serial(documents, cache_path=TOKENIZED_PATH):\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"r\", encoding='utf-8') as f:\n",
    "            print(\"æ£€æµ‹åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½åˆ†è¯ç»“æœ...\")\n",
    "            return json.load(f)\n",
    "\n",
    "    print(\"æ­£åœ¨è¿›è¡Œåˆ†è¯...\")\n",
    "    ids, texts = zip(*documents.items())\n",
    "    tokenized = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for doc_id, text in tqdm(zip(ids, texts), total=len(ids), desc=\"åˆ†è¯è¿›åº¦\"):\n",
    "        tokenized[doc_id] = jieba_cut_serial(text)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    with open(cache_path, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(tokenized, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"åˆ†è¯å®Œæˆï¼Œå…±{len(tokenized)}ç¯‡æ–‡æ¡£ï¼Œè€—æ—¶{elapsed:.2f}ç§’\")\n",
    "    return tokenized\n",
    "\n",
    "documents = load_documents(ARTICLE_DIR) \n",
    "tokenized_docs = tokenize_serial(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8657d36",
   "metadata": {},
   "source": [
    "### BERTæ¨¡å‹åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1a3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zzsyp\\anaconda3\\envs\\lanxin\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\zzsyp\\anaconda3\\envs\\lanxin\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTæ¨¡å‹å·²åŠ è½½åˆ°GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch \n",
    "\n",
    "class ChineseWordEncoder:\n",
    "    def __init__(self, model_name=\"hfl/chinese-roberta-wwm-ext\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶å°†å…¶æ”¾åˆ°GPUä¸Š\n",
    "        self.model = BertModel.from_pretrained(model_name).to(\"cuda\")\n",
    "        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        # ä½¿ç”¨ tokenizer å¯¹è¯è¯­è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆ token id å½¢å¼çš„è¾“å…¥\n",
    "        # truncation=True è¡¨ç¤ºå¯¹è¿‡é•¿è¾“å…¥è‡ªåŠ¨æˆªæ–­ï¼›max_length=10 æ˜¯ä¸ºäº†å®‰å…¨é™åˆ¶é•¿åº¦\n",
    "        # return_tensors=\"pt\" ä¼šè¿”å› PyTorch å¼ é‡å½¢å¼çš„è¾“å…¥ \n",
    "        inputs = self.tokenizer(word, return_tensors=\"pt\", truncation=True, max_length=10).to(\"cuda\")\n",
    "        # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ˆåŠ é€Ÿ + å‡å°‘æ˜¾å­˜å ç”¨ï¼‰ï¼Œæ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–è¾“å‡ºç»“æœ\n",
    "        with torch.no_grad():\n",
    "            # outputs æ˜¯ä¸€ä¸ªåŒ…å« last_hidden_state ç­‰å¤šä¸ªè¾“å‡ºçš„å¯¹è±¡\n",
    "            outputs = self.model(**inputs)\n",
    "        # ä»è¾“å‡ºä¸­æå– `last_hidden_state`ï¼Œå½¢çŠ¶ä¸º (batch_size=1, sequence_len, hidden_dim)\n",
    "        # æˆ‘ä»¬é€‰å–ç¬¬ä¸€ä¸ª tokenï¼ˆCLS æ ‡è®°ï¼‰å¯¹åº”çš„å‘é‡è¡¨ç¤ºï¼Œä½œä¸ºæ•´ä½“è¯è¯­çš„è¯­ä¹‰è¡¨è¾¾\n",
    "        cls_vector = outputs.last_hidden_state[:, 0, :]  # å– batch çš„ç¬¬ä¸€ä¸ª token çš„å…¨éƒ¨éšè—å±‚å‘é‡\n",
    "\n",
    "        # squeeze(0)ï¼šå°† (1, 768) => (768,)\n",
    "        # cpu().numpy()ï¼šå°†å¼ é‡ä» GPU ç§»è‡³ CPU å¹¶è½¬ä¸º NumPy æ•°ç»„\n",
    "        return cls_vector.squeeze(0).cpu().numpy()\n",
    "\n",
    "encoder = ChineseWordEncoder()\n",
    "print(\"BERTæ¨¡å‹å·²åŠ è½½åˆ°GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef71a42",
   "metadata": {},
   "source": [
    "### SQLiteæ•°æ®åº“æœ‰å…³æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6253763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# åˆå§‹åŒ– SQLite æ•°æ®åº“\n",
    "def init_sqlite_vector_db(db_path=SQLITE_VECTOR_DB):\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_vectors (\n",
    "            word TEXT PRIMARY KEY,\n",
    "            vector BLOB\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# å†™å…¥è¯å‘é‡\n",
    "def save_vector_to_sqlite(word, vector, db_path=SQLITE_VECTOR_DB):\n",
    "    vec_bytes = vector.astype(\"float32\").tobytes()\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"INSERT OR REPLACE INTO word_vectors (word, vector) VALUES (?, ?)\", (word, vec_bytes))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# è¯»å–è¯å‘é‡\n",
    "def load_vector_from_sqlite(word, db_path=SQLITE_VECTOR_DB):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT vector FROM word_vectors WHERE word = ?\", (word,))\n",
    "    row = c.fetchone()\n",
    "    conn.close()\n",
    "    if row is None:\n",
    "        return None\n",
    "    return np.frombuffer(row[0], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8225f",
   "metadata": {},
   "source": [
    "### å€’æ’ç´¢å¼• + è·³è¡¨äº¤é›† + SQLite + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee591aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "from collections import defaultdict\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(list) \n",
    "        self.word_list = [] \n",
    "        self.faiss_index = None\n",
    "\n",
    "    def build(self, tokenized_docs, encoder, db_path=SQLITE_VECTOR_DB):\n",
    "        # å¦‚æœç¼“å­˜æ–‡ä»¶éƒ½å­˜åœ¨ï¼Œç›´æ¥åŠ è½½\n",
    "        if os.path.exists(WORD_LIST_PATH) and os.path.exists(FAISS_INDEX_PATH):\n",
    "            with open(WORD_LIST_PATH, \"r\", encoding='utf-8') as f:\n",
    "                self.word_list = json.load(f)\n",
    "            self.faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "            print(f\"å·²ä»ç¼“å­˜åŠ è½½ word_list å’Œ faiss.indexï¼Œè¯æ•°ï¼š{len(self.word_list)}\")\n",
    "            \n",
    "            # åŒæ—¶ä»éœ€é‡å»ºå€’æ’ç´¢å¼•ï¼ˆæ¥è‡ª tokenized_docsï¼‰\n",
    "            for doc_id, tokens in tokenized_docs.items():\n",
    "                for token in set(tokens):\n",
    "                    self.index[token].append(int(doc_id))\n",
    "            for token in self.index:\n",
    "                self.index[token].sort()\n",
    "            return  # ç›´æ¥ç»“æŸï¼Œé¿å…åç»­å†—ä½™æ­¥éª¤\n",
    "        \n",
    "        # åˆå§‹åŒ– SQLite æ•°æ®åº“\n",
    "        init_sqlite_vector_db(db_path)\n",
    "\n",
    "        # 1. æ„å»ºå€’æ’ç´¢å¼• + è¯è¡¨\n",
    "        vocab = set()\n",
    "        for doc_id, tokens in tokenized_docs.items():\n",
    "            # set() å»é‡\n",
    "            for token in set(tokens):\n",
    "                self.index[token].append(int(doc_id))\n",
    "                vocab.add(token)\n",
    "        self.word_list = list(self.vocab)\n",
    "\n",
    "        # 2. å‘é‡æ„å»ºå¹¶å†™å…¥ SQLiteï¼ŒåŒæ—¶æ„å»ºå‘é‡é›†åˆç”¨äº FAISS\n",
    "        vecs = []\n",
    "        word_vectors = {}\n",
    "        for word in self.word_list:\n",
    "            vec = load_vector_from_sqlite(word, db_path)\n",
    "            if vec is None:\n",
    "                try:\n",
    "                    vec = encoder.get_vector(word)\n",
    "                    save_vector_to_sqlite(word, vec, db_path)\n",
    "                except:\n",
    "                    vec = np.zeros(768)\n",
    "                    save_vector_to_sqlite(word, vec, db_path)\n",
    "            word_vectors[word] = vec.tolist()\n",
    "            vecs.append(vec)\n",
    "\n",
    "        # 3. æ„å»º FAISS ç´¢å¼•\n",
    "        vecs = np.array(vecs).astype(\"float32\")\n",
    "        self.faiss_index = faiss.IndexFlatL2(vecs.shape[1])\n",
    "        self.faiss_index.add(vecs)\n",
    "\n",
    "        # 4. ä¿å­˜ word_list å’Œ FAISS ç´¢å¼•\n",
    "        with open(WORD_LIST_PATH, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(self.word_list, f)\n",
    "        faiss.write_index(self.faiss_index, FAISS_INDEX_PATH)\n",
    "\n",
    "        # 5. å¯¹å€’æ’ç´¢å¼•æŒ‰æ–‡æ¡£æ’åº\n",
    "        for token in self.index:\n",
    "            self.index[token].sort()\n",
    "\n",
    "        print(f\"å€’æ’ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±ç´¢å¼•è¯æ•°ï¼š{len(self.word_list)}ï¼Œè¯å‘é‡ç¼“å­˜äºSQLiteã€‚\")\n",
    "    \n",
    "    \n",
    "    def intersect_with_skip(self, list1, list2):\n",
    "        if not list1 or not list2:\n",
    "            return []\n",
    "\n",
    "        result = []\n",
    "        i, j = 0, 0\n",
    "        len1, len2 = len(list1), len(list2)\n",
    "        skip1 = int(len1 ** 0.5) or 1\n",
    "        skip2 = int(len2 ** 0.5) or 1\n",
    "\n",
    "        while i < len1 and j < len2:\n",
    "            if list1[i] == list2[j]:\n",
    "                result.append(list1[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif list1[i] < list2[j]:\n",
    "                next_i = i + skip1\n",
    "                if next_i < len1 and list1[next_i] <= list2[j]:\n",
    "                    i = next_i\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                next_j = j + skip2\n",
    "                if next_j < len2 and list2[next_j] <= list1[i]:\n",
    "                    j = next_j\n",
    "                else:\n",
    "                    j += 1\n",
    "\n",
    "        return result\n",
    "    \n",
    "    # Top-Kç›¸ä¼¼è¯æŸ¥è¯¢å‡½æ•°\n",
    "    def top_k_similar(self, query_vec, query_word=None, k=2):\n",
    "        query_vec = np.array([query_vec]).astype(\"float32\")\n",
    "        distances, indices = self.faiss_index.search(query_vec, k + 1)\n",
    "        candidates = []\n",
    "        for i, d in zip(indices[0], distances[0]):\n",
    "            word = self.word_list[i]\n",
    "            if word == query_word:\n",
    "                continue\n",
    "            candidates.append((word, d))\n",
    "            if len(candidates) == k:\n",
    "                break\n",
    "        return candidates\n",
    "\n",
    "    # æœç´¢+TOP KæŸ¥è¯¢å‡½æ•°\n",
    "    def search(self, word, encoder, topk=2):\n",
    "        start_time = time.time()\n",
    "        docs_word = self.index.get(word, [])\n",
    "        try:\n",
    "            word_vec = encoder.get_vector(word)\n",
    "        except:\n",
    "            return [], [], [], 0.0\n",
    "\n",
    "        top_similar = self.top_k_similar(word_vec, word, k=topk)\n",
    "        docs_similars = [self.index.get(sim_word, []) for sim_word, _ in top_similar]\n",
    "        if len(docs_similars) < 2:\n",
    "            return [], docs_word, top_similar, time.time() - start_time\n",
    "\n",
    "        all_lists = [docs_word] + docs_similars\n",
    "        all_lists = sorted(all_lists, key=len)\n",
    "        temp = self.intersect_with_skip(all_lists[0], all_lists[1])\n",
    "        final_result = self.intersect_with_skip(temp, all_lists[2])\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"\\n=== ğŸ”æŸ¥è¯¢å…³é”®è¯ï¼š{word} ===\")\n",
    "        print(f\"æŸ¥è¯¢è¯åŒ…å«æ–‡æ¡£æ•°: {len(docs_word)}ï¼Œç¤ºä¾‹:{docs_word[:10]}\")\n",
    "        print(f\"Top{topk}ç›¸ä¼¼è¯:\")\n",
    "        for i, (sim_word, dist) in enumerate(top_similar, 1):\n",
    "            sim_docs = self.index.get(sim_word, [])\n",
    "            print(f\" {i}. {sim_word} (L2è·ç¦»{dist:.2f})ï¼Œå‡ºç°äº{len(sim_docs)}ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:{sim_docs[:10]}\")\n",
    "        print(f\"äº¤é›†æ–‡æ¡£æ•°: {len(final_result)}ï¼Œç¤ºä¾‹:{final_result[:10]}\")\n",
    "        print(f\"æŸ¥è¯¢è€—æ—¶: {elapsed:.3f}ç§’\")\n",
    "\n",
    "        return final_result, docs_word, top_similar, elapsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65404d45",
   "metadata": {},
   "source": [
    "### æµ‹è¯•æ ·ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670c73f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²ä»ç¼“å­˜åŠ è½½ word_list å’Œ faiss.indexï¼Œè¯æ•°ï¼š88370\n",
      "\n",
      "=== ğŸ”æŸ¥è¯¢å…³é”®è¯ï¼šäº’è”ç½‘ ===\n",
      "æŸ¥è¯¢è¯åŒ…å«æ–‡æ¡£æ•°: 439ï¼Œç¤ºä¾‹:[8, 93, 106, 122, 126, 163, 180, 221, 235, 251]\n",
      "Top2ç›¸ä¼¼è¯:\n",
      " 1. äº’è”ç½‘ç»œ (L2è·ç¦»28.31)ï¼Œå‡ºç°äº2ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[15433, 19630]\n",
      " 2. äº’è”ç½‘ç«™ (L2è·ç¦»28.79)ï¼Œå‡ºç°äº1ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[9514]\n",
      "äº¤é›†æ–‡æ¡£æ•°: 0ï¼Œç¤ºä¾‹:[]\n",
      "æŸ¥è¯¢è€—æ—¶: 0.573ç§’\n",
      "\n",
      "=== ğŸ”æŸ¥è¯¢å…³é”®è¯ï¼šç»æµ ===\n",
      "æŸ¥è¯¢è¯åŒ…å«æ–‡æ¡£æ•°: 3040ï¼Œç¤ºä¾‹:[2, 5, 13, 28, 29, 30, 32, 44, 47, 65]\n",
      "Top2ç›¸ä¼¼è¯:\n",
      " 1. éç»æµ (L2è·ç¦»40.02)ï¼Œå‡ºç°äº1ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[17892]\n",
      " 2. ç»æµè´¦ (L2è·ç¦»40.52)ï¼Œå‡ºç°äº2ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[10483, 18906]\n",
      "äº¤é›†æ–‡æ¡£æ•°: 0ï¼Œç¤ºä¾‹:[]\n",
      "æŸ¥è¯¢è€—æ—¶: 0.033ç§’\n",
      "\n",
      "=== ğŸ”æŸ¥è¯¢å…³é”®è¯ï¼šç¾å›½ ===\n",
      "æŸ¥è¯¢è¯åŒ…å«æ–‡æ¡£æ•°: 1752ï¼Œç¤ºä¾‹:[28, 39, 62, 67, 77, 110, 126, 136, 140, 152]\n",
      "Top2ç›¸ä¼¼è¯:\n",
      " 1. ç¾ç­‰å›½ (L2è·ç¦»36.43)ï¼Œå‡ºç°äº1ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[29]\n",
      " 2. ç¾ä¸¤å›½ (L2è·ç¦»39.21)ï¼Œå‡ºç°äº4ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[4373, 16508, 18173, 18192]\n",
      "äº¤é›†æ–‡æ¡£æ•°: 0ï¼Œç¤ºä¾‹:[]\n",
      "æŸ¥è¯¢è€—æ—¶: 0.035ç§’\n",
      "\n",
      "=== ğŸ”æŸ¥è¯¢å…³é”®è¯ï¼šæ¶ˆè´¹ ===\n",
      "æŸ¥è¯¢è¯åŒ…å«æ–‡æ¡£æ•°: 265ï¼Œç¤ºä¾‹:[90, 140, 250, 311, 381, 395, 415, 513, 538, 547]\n",
      "Top2ç›¸ä¼¼è¯:\n",
      " 1. æ¶ˆè´¹è§‚ (L2è·ç¦»43.97)ï¼Œå‡ºç°äº1ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[10196]\n",
      " 2. æ¶ˆè´¹é‡ (L2è·ç¦»44.27)ï¼Œå‡ºç°äº8ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[3433, 4317, 10131, 10196, 11781, 11782, 17302, 18685]\n",
      "äº¤é›†æ–‡æ¡£æ•°: 0ï¼Œç¤ºä¾‹:[]\n",
      "æŸ¥è¯¢è€—æ—¶: 0.035ç§’\n",
      "\n",
      "=== ğŸ”æŸ¥è¯¢å…³é”®è¯ï¼šå†›é˜Ÿ ===\n",
      "æŸ¥è¯¢è¯åŒ…å«æ–‡æ¡£æ•°: 396ï¼Œç¤ºä¾‹:[2, 5, 91, 106, 144, 156, 178, 308, 453, 528]\n",
      "Top2ç›¸ä¼¼è¯:\n",
      " 1. å†›äºº (L2è·ç¦»31.02)ï¼Œå‡ºç°äº124ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[453, 549, 605, 643, 655, 673, 762, 800, 1627, 1778]\n",
      " 2. éƒ¨é˜Ÿ (L2è·ç¦»31.51)ï¼Œå‡ºç°äº477ç¯‡æ–‡æ¡£ï¼Œç¤ºä¾‹:[1, 2, 15, 91, 144, 178, 183, 316, 371, 404]\n",
      "äº¤é›†æ–‡æ¡£æ•°: 10ï¼Œç¤ºä¾‹:[453, 3818, 3887, 3919, 3960, 7723, 11399, 13479, 16498, 20654]\n",
      "æŸ¥è¯¢è€—æ—¶: 0.033ç§’\n"
     ]
    }
   ],
   "source": [
    "index = InvertedIndex()\n",
    "index.build(tokenized_docs, encoder)\n",
    "\n",
    "query_words = [\"äº’è”ç½‘\", \"ç»æµ\", \"ç¾å›½\", \"æ¶ˆè´¹\", \"å†›é˜Ÿ\"]\n",
    "results = {}\n",
    "\n",
    "for query in query_words:\n",
    "    result_docs, base_docs, top2, used_time = index.search(query, encoder)\n",
    "    results[query] = result_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3cbeee",
   "metadata": {},
   "source": [
    "### åŠŸèƒ½æ‰©å±• å¤æ‚å¸ƒå°”è¡¨è¾¾å¼æŸ¥è¯¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604de47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¼˜å…ˆçº§è®¾å®š\n",
    "PRECEDENCE = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "\n",
    "def tokenize(expr):\n",
    "    # æ‹¬å·å‰ååŠ ç©ºæ ¼ï¼Œæ–¹ä¾¿åˆ†éš”\n",
    "    expr = expr.replace('(', ' ( ').replace(')', ' ) ')\n",
    "    return expr.strip().split()\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    output = []\n",
    "    stack = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()  # å¼¹å‡º (\n",
    "        elif token.upper() in PRECEDENCE:\n",
    "            while (stack and stack[-1] != '(' and\n",
    "                   PRECEDENCE.get(stack[-1].upper(), 0) >= PRECEDENCE[token.upper()]):\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token.upper())\n",
    "        else:\n",
    "            output.append(token)\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "def eval_postfix(postfix_tokens, index, all_ids):\n",
    "    stack = []\n",
    "    for token in postfix_tokens:\n",
    "        token = token.upper()\n",
    "        if token == 'NOT':\n",
    "            if not stack:\n",
    "                raise ValueError(\"æ ˆä¸ºç©ºï¼ŒNOT ç¼ºå°‘æ“ä½œæ•°\")\n",
    "            operand = stack.pop()\n",
    "            stack.append(all_ids - operand)\n",
    "        elif token in ('AND', 'OR'):\n",
    "            b = stack.pop()\n",
    "            a = stack.pop()\n",
    "            result = a & b if token == 'AND' else a | b\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            stack.append(set(index.index.get(token, [])))\n",
    "    if len(stack) != 1:\n",
    "        raise ValueError(\"åç¼€è¡¨è¾¾å¼æ±‚å€¼é”™è¯¯ï¼æ£€æŸ¥è¡¨è¾¾å¼\")\n",
    "    return stack[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053e0662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥è¡¨è¾¾å¼: NOT ç»æµ AND (äº’è”ç½‘ AND æ¶ˆè´¹) \n",
      "åç¼€è¡¨è¾¾å¼: ['ç»æµ', 'NOT', 'äº’è”ç½‘', 'æ¶ˆè´¹', 'AND', 'AND']\n",
      "åŒ¹é…æ–‡æ¡£æ•°: 4\n",
      "æ–‡æ¡£IDæ ·ä¾‹: [19658, 18827, 15702, 18847]\n",
      "è¾“å…¥è¡¨è¾¾å¼: ç»æµ AND äº’è”ç½‘ AND æ¶ˆè´¹\n",
      "åç¼€è¡¨è¾¾å¼: ['ç»æµ', 'äº’è”ç½‘', 'AND', 'æ¶ˆè´¹', 'AND']\n",
      "åŒ¹é…æ–‡æ¡£æ•°: 21\n",
      "æ–‡æ¡£IDæ ·ä¾‹: [18179, 1672, 18700, 6960, 14388, 5430, 15418, 16700, 3260, 1472]\n",
      "è¾“å…¥è¡¨è¾¾å¼: äº’è”ç½‘ AND æ¶ˆè´¹\n",
      "åç¼€è¡¨è¾¾å¼: ['äº’è”ç½‘', 'æ¶ˆè´¹', 'AND']\n",
      "åŒ¹é…æ–‡æ¡£æ•°: 25\n",
      "æ–‡æ¡£IDæ ·ä¾‹: [18179, 1672, 18827, 18700, 18847, 6960, 14388, 5430, 15418, 16700]\n"
     ]
    }
   ],
   "source": [
    "exprs = [\"NOT ç»æµ AND (äº’è”ç½‘ AND æ¶ˆè´¹) \" ,\n",
    "         \"ç»æµ AND äº’è”ç½‘ AND æ¶ˆè´¹\",\n",
    "         \"äº’è”ç½‘ AND æ¶ˆè´¹\"\n",
    "]\n",
    "# æ„é€ å…¨é›†\n",
    "all_ids = set()\n",
    "for docs in index.index.values():\n",
    "    all_ids.update(docs)\n",
    "\n",
    "for expr in exprs:\n",
    "    print(\"è¾“å…¥è¡¨è¾¾å¼:\", expr)\n",
    "    tokens = tokenize(expr)\n",
    "    postfix = infix_to_postfix(tokens)\n",
    "    print(\"åç¼€è¡¨è¾¾å¼:\", postfix) \n",
    "    result = eval_postfix(postfix, index, all_ids)\n",
    "    print(\"åŒ¹é…æ–‡æ¡£æ•°:\", len(result))\n",
    "    print(\"æ–‡æ¡£IDæ ·ä¾‹:\", list(result)[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lanxin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
